{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"gcn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUv7M0dIhLA2","outputId":"29c6882f-0187-4038-82d8-79a7183c5e10","executionInfo":{"status":"ok","timestamp":1642730037085,"user_tz":-540,"elapsed":30677,"user":{"displayName":"Soyoung Lee","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00392056614469375972"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","\n","import math\n","import torch\n","\n","import numpy as np\n","import scipy.sparse as sp\n","import torch.optim as optim\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module"],"metadata":{"id":"KIdGIgUfjO26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#layers\n","class GraphConvolution(Module):\n","    \"\"\"\n","    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(GraphConvolution, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n","        if bias:\n","            self.bias = Parameter(torch.FloatTensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-stdv, stdv)\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, input, adj):\n","        support = torch.mm(input, self.weight)\n","        output = torch.spmm(adj, support)\n","        if self.bias is not None:\n","            return output + self.bias\n","        else:\n","            return output\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + ' (' \\\n","               + str(self.in_features) + ' -> ' \\\n","               + str(self.out_features) + ')'"],"metadata":{"id":"0pXsGVlQkI_U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model\n","\n","class GCN(nn.Module):\n","    def __init__(self, nfeat, nhid, nclass, dropout):\n","        super(GCN, self).__init__()\n","\n","        self.gc1 = GraphConvolution(nfeat, nhid)\n","        self.gc2 = GraphConvolution(nhid, nclass)\n","        self.dropout = dropout\n","\n","    def forward(self, x, adj):\n","        x = F.relu(self.gc1(x, adj))\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        x = self.gc2(x, adj)\n","        return F.log_softmax(x, dim=1)"],"metadata":{"id":"RkVgJet0kXpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#utils\n","def encode_onehot(labels):\n","    classes = set(labels)\n","    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n","                    enumerate(classes)}\n","    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n","                             dtype=np.int32)\n","    return labels_onehot\n","\n","\n","def load_data(path=\"/content/drive/MyDrive/data/cora/\", dataset=\"cora\"):\n","    print('Loading {} dataset...'.format(dataset))\n","\n","    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n","                                        dtype=np.dtype(str))\n","    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n","    labels = encode_onehot(idx_features_labels[:, -1])\n","\n","    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n","    idx_map = {j: i for i, j in enumerate(idx)}\n","    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n","                                    dtype=np.int32)\n","    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n","                     dtype=np.int32).reshape(edges_unordered.shape)\n","    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n","                        shape=(labels.shape[0], labels.shape[0]),\n","                        dtype=np.float32)\n","\n","    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n","\n","    features = normalize(features)\n","    adj = normalize(adj + sp.eye(adj.shape[0]))\n","\n","    idx_train = range(140)\n","    idx_val = range(200, 500)\n","    idx_test = range(500, 1500)\n","\n","    features = torch.FloatTensor(np.array(features.todense()))\n","    labels = torch.LongTensor(np.where(labels)[1])\n","    adj = sparse_mx_to_torch_sparse_tensor(adj)\n","\n","    idx_train = torch.LongTensor(idx_train)\n","    idx_val = torch.LongTensor(idx_val)\n","    idx_test = torch.LongTensor(idx_test)\n","\n","    return adj, features, labels, idx_train, idx_val, idx_test\n","\n","def normalize(mx):\n","    \"\"\"Row-normalize sparse matrix\"\"\"\n","    rowsum = np.array(mx.sum(1))\n","    r_inv = np.power(rowsum, -1).flatten()\n","    r_inv[np.isinf(r_inv)] = 0.\n","    r_mat_inv = sp.diags(r_inv)\n","    mx = r_mat_inv.dot(mx)\n","    return mx\n","\n","def accuracy(output, labels):\n","    preds = output.max(1)[1].type_as(labels)\n","    correct = preds.eq(labels).double()\n","    correct = correct.sum()\n","    return correct / len(labels)\n","\n","def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n","    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n","    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n","    indices = torch.from_numpy(\n","        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n","    values = torch.from_numpy(sparse_mx.data)\n","    shape = torch.Size(sparse_mx.shape)\n","    return torch.sparse.FloatTensor(indices, values, shape)"],"metadata":{"id":"H8aZKCUxkcjB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Training\n","import easydict\n","\n","args = easydict.EasyDict({\"no-cuda\":False, \"fastmode\":False, \"seed\":42, \\\n","                          \"epochs\":200, \"lr\":0.01, \"weight_decay\":5e-4, \\\n","                          \"hidden\":16, \"dropout\":0.5, \"cuda\":True})\n","\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","\n","# Load data\n","adj, features, labels, idx_train, idx_val, idx_test = load_data()\n","\n","# Model and optimizer\n","model = GCN(nfeat=features.shape[1],\n","            nhid=args.hidden,\n","            nclass=labels.max().item() + 1,\n","            dropout=args.dropout)\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=args.lr, weight_decay=args.weight_decay)\n","\n","if args.cuda:\n","    model.cuda()\n","    features = features.cuda()\n","    adj = adj.cuda()\n","    labels = labels.cuda()\n","    idx_train = idx_train.cuda()\n","    idx_val = idx_val.cuda()\n","    idx_test = idx_test.cuda()\n","\n","\n","def train(epoch):\n","    t = time.time()\n","    model.train()\n","    optimizer.zero_grad()\n","    output = model(features, adj)\n","    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n","    acc_train = accuracy(output[idx_train], labels[idx_train])\n","    loss_train.backward()\n","    optimizer.step()\n","\n","    if not args.fastmode:\n","        # Evaluate validation set performance separately,\n","        # deactivates dropout during validation run.\n","        model.eval()\n","        output = model(features, adj)\n","\n","    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n","    acc_val = accuracy(output[idx_val], labels[idx_val])\n","    print('Epoch: {:04d}'.format(epoch+1),\n","          'loss_train: {:.4f}'.format(loss_train.item()),\n","          'acc_train: {:.4f}'.format(acc_train.item()),\n","          'loss_val: {:.4f}'.format(loss_val.item()),\n","          'acc_val: {:.4f}'.format(acc_val.item()),\n","          'time: {:.4f}s'.format(time.time() - t))\n","\n","\n","def test():\n","    model.eval()\n","    output = model(features, adj)\n","    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n","    acc_test = accuracy(output[idx_test], labels[idx_test])\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test.item()))\n","\n","\n","# Train model\n","t_total = time.time()\n","for epoch in range(args.epochs):\n","    train(epoch)\n","print(\"Optimization Finished!\")\n","print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n","\n","# Testing\n","test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBwxO0WZlJOb","outputId":"be4c7913-9118-45b4-861d-f7f5950b22e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cora dataset...\n","Epoch: 0001 loss_train: 1.9538 acc_train: 0.1000 loss_val: 1.9324 acc_val: 0.0867 time: 0.1777s\n","Epoch: 0002 loss_train: 1.9338 acc_train: 0.1429 loss_val: 1.9191 acc_val: 0.1400 time: 0.0070s\n","Epoch: 0003 loss_train: 1.9153 acc_train: 0.2143 loss_val: 1.9064 acc_val: 0.3433 time: 0.0058s\n","Epoch: 0004 loss_train: 1.9091 acc_train: 0.2643 loss_val: 1.8942 acc_val: 0.3500 time: 0.0063s\n","Epoch: 0005 loss_train: 1.8953 acc_train: 0.2929 loss_val: 1.8826 acc_val: 0.3500 time: 0.0071s\n","Epoch: 0006 loss_train: 1.8858 acc_train: 0.3000 loss_val: 1.8718 acc_val: 0.3500 time: 0.0066s\n","Epoch: 0007 loss_train: 1.8806 acc_train: 0.2929 loss_val: 1.8620 acc_val: 0.3500 time: 0.0064s\n","Epoch: 0008 loss_train: 1.8711 acc_train: 0.2929 loss_val: 1.8527 acc_val: 0.3500 time: 0.0069s\n","Epoch: 0009 loss_train: 1.8650 acc_train: 0.2929 loss_val: 1.8437 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0010 loss_train: 1.8469 acc_train: 0.2929 loss_val: 1.8348 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0011 loss_train: 1.8339 acc_train: 0.2929 loss_val: 1.8260 acc_val: 0.3500 time: 0.0065s\n","Epoch: 0012 loss_train: 1.8265 acc_train: 0.2929 loss_val: 1.8173 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0013 loss_train: 1.8204 acc_train: 0.2929 loss_val: 1.8087 acc_val: 0.3500 time: 0.0069s\n","Epoch: 0014 loss_train: 1.8220 acc_train: 0.2929 loss_val: 1.8005 acc_val: 0.3500 time: 0.0069s\n","Epoch: 0015 loss_train: 1.8089 acc_train: 0.2929 loss_val: 1.7924 acc_val: 0.3500 time: 0.0069s\n","Epoch: 0016 loss_train: 1.7945 acc_train: 0.2929 loss_val: 1.7845 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0017 loss_train: 1.7788 acc_train: 0.2929 loss_val: 1.7769 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0018 loss_train: 1.7754 acc_train: 0.2929 loss_val: 1.7693 acc_val: 0.3500 time: 0.0068s\n","Epoch: 0019 loss_train: 1.7469 acc_train: 0.2929 loss_val: 1.7618 acc_val: 0.3500 time: 0.0068s\n","Epoch: 0020 loss_train: 1.7678 acc_train: 0.2929 loss_val: 1.7545 acc_val: 0.3500 time: 0.0067s\n","Epoch: 0021 loss_train: 1.7588 acc_train: 0.2929 loss_val: 1.7475 acc_val: 0.3500 time: 0.0068s\n","Epoch: 0022 loss_train: 1.7494 acc_train: 0.2929 loss_val: 1.7409 acc_val: 0.3500 time: 0.0069s\n","Epoch: 0023 loss_train: 1.7406 acc_train: 0.3000 loss_val: 1.7345 acc_val: 0.3500 time: 0.0068s\n","Epoch: 0024 loss_train: 1.7063 acc_train: 0.2929 loss_val: 1.7284 acc_val: 0.3500 time: 0.0069s\n","Epoch: 0025 loss_train: 1.7205 acc_train: 0.3000 loss_val: 1.7225 acc_val: 0.3500 time: 0.0068s\n","Epoch: 0026 loss_train: 1.7331 acc_train: 0.2929 loss_val: 1.7168 acc_val: 0.3500 time: 0.0056s\n","Epoch: 0027 loss_train: 1.6967 acc_train: 0.3000 loss_val: 1.7112 acc_val: 0.3500 time: 0.0068s\n","Epoch: 0028 loss_train: 1.6967 acc_train: 0.2929 loss_val: 1.7055 acc_val: 0.3500 time: 0.0069s\n","Epoch: 0029 loss_train: 1.7050 acc_train: 0.3000 loss_val: 1.6998 acc_val: 0.3500 time: 0.0070s\n","Epoch: 0030 loss_train: 1.6674 acc_train: 0.3357 loss_val: 1.6939 acc_val: 0.3500 time: 0.0115s\n","Epoch: 0031 loss_train: 1.6874 acc_train: 0.3143 loss_val: 1.6879 acc_val: 0.3500 time: 0.0055s\n","Epoch: 0032 loss_train: 1.6567 acc_train: 0.3071 loss_val: 1.6815 acc_val: 0.3533 time: 0.0070s\n","Epoch: 0033 loss_train: 1.6671 acc_train: 0.3357 loss_val: 1.6749 acc_val: 0.3567 time: 0.0069s\n","Epoch: 0034 loss_train: 1.6642 acc_train: 0.3214 loss_val: 1.6682 acc_val: 0.3633 time: 0.0070s\n","Epoch: 0035 loss_train: 1.6183 acc_train: 0.3571 loss_val: 1.6612 acc_val: 0.3633 time: 0.0070s\n","Epoch: 0036 loss_train: 1.6561 acc_train: 0.3500 loss_val: 1.6542 acc_val: 0.3667 time: 0.0070s\n","Epoch: 0037 loss_train: 1.6378 acc_train: 0.3929 loss_val: 1.6469 acc_val: 0.3667 time: 0.0069s\n","Epoch: 0038 loss_train: 1.6024 acc_train: 0.4143 loss_val: 1.6393 acc_val: 0.3767 time: 0.0068s\n","Epoch: 0039 loss_train: 1.5819 acc_train: 0.3857 loss_val: 1.6314 acc_val: 0.3800 time: 0.0067s\n","Epoch: 0040 loss_train: 1.5634 acc_train: 0.4000 loss_val: 1.6230 acc_val: 0.3900 time: 0.0050s\n","Epoch: 0041 loss_train: 1.5596 acc_train: 0.4286 loss_val: 1.6142 acc_val: 0.4000 time: 0.0070s\n","Epoch: 0042 loss_train: 1.5650 acc_train: 0.4214 loss_val: 1.6049 acc_val: 0.4000 time: 0.0067s\n","Epoch: 0043 loss_train: 1.5470 acc_train: 0.4429 loss_val: 1.5954 acc_val: 0.4000 time: 0.0065s\n","Epoch: 0044 loss_train: 1.5170 acc_train: 0.4286 loss_val: 1.5855 acc_val: 0.4033 time: 0.0065s\n","Epoch: 0045 loss_train: 1.4909 acc_train: 0.4714 loss_val: 1.5754 acc_val: 0.4100 time: 0.0068s\n","Epoch: 0046 loss_train: 1.4835 acc_train: 0.4500 loss_val: 1.5650 acc_val: 0.4133 time: 0.0068s\n","Epoch: 0047 loss_train: 1.4684 acc_train: 0.4429 loss_val: 1.5544 acc_val: 0.4133 time: 0.0068s\n","Epoch: 0048 loss_train: 1.4549 acc_train: 0.4500 loss_val: 1.5437 acc_val: 0.4133 time: 0.0062s\n","Epoch: 0049 loss_train: 1.4535 acc_train: 0.4571 loss_val: 1.5329 acc_val: 0.4133 time: 0.0067s\n","Epoch: 0050 loss_train: 1.4399 acc_train: 0.4786 loss_val: 1.5221 acc_val: 0.4133 time: 0.0087s\n","Epoch: 0051 loss_train: 1.4184 acc_train: 0.4857 loss_val: 1.5113 acc_val: 0.4133 time: 0.0067s\n","Epoch: 0052 loss_train: 1.4156 acc_train: 0.4643 loss_val: 1.5004 acc_val: 0.4233 time: 0.0061s\n","Epoch: 0053 loss_train: 1.4226 acc_train: 0.4857 loss_val: 1.4893 acc_val: 0.4300 time: 0.0066s\n","Epoch: 0054 loss_train: 1.3815 acc_train: 0.4786 loss_val: 1.4779 acc_val: 0.4333 time: 0.0056s\n","Epoch: 0055 loss_train: 1.3647 acc_train: 0.4929 loss_val: 1.4665 acc_val: 0.4367 time: 0.0066s\n","Epoch: 0056 loss_train: 1.3745 acc_train: 0.4857 loss_val: 1.4550 acc_val: 0.4433 time: 0.0064s\n","Epoch: 0057 loss_train: 1.3466 acc_train: 0.5071 loss_val: 1.4438 acc_val: 0.4567 time: 0.0065s\n","Epoch: 0058 loss_train: 1.3296 acc_train: 0.5000 loss_val: 1.4325 acc_val: 0.4667 time: 0.0054s\n","Epoch: 0059 loss_train: 1.2840 acc_train: 0.5857 loss_val: 1.4214 acc_val: 0.4800 time: 0.0080s\n","Epoch: 0060 loss_train: 1.3064 acc_train: 0.5643 loss_val: 1.4104 acc_val: 0.5000 time: 0.0063s\n","Epoch: 0061 loss_train: 1.2881 acc_train: 0.5571 loss_val: 1.3995 acc_val: 0.5000 time: 0.0063s\n","Epoch: 0062 loss_train: 1.2673 acc_train: 0.5429 loss_val: 1.3886 acc_val: 0.5133 time: 0.0065s\n","Epoch: 0063 loss_train: 1.2677 acc_train: 0.5571 loss_val: 1.3778 acc_val: 0.5200 time: 0.0066s\n","Epoch: 0064 loss_train: 1.2455 acc_train: 0.5929 loss_val: 1.3669 acc_val: 0.5367 time: 0.0066s\n","Epoch: 0065 loss_train: 1.2226 acc_train: 0.6071 loss_val: 1.3561 acc_val: 0.5567 time: 0.0066s\n","Epoch: 0066 loss_train: 1.2197 acc_train: 0.5643 loss_val: 1.3451 acc_val: 0.5600 time: 0.0058s\n","Epoch: 0067 loss_train: 1.1922 acc_train: 0.6071 loss_val: 1.3342 acc_val: 0.5833 time: 0.0069s\n","Epoch: 0068 loss_train: 1.2103 acc_train: 0.6071 loss_val: 1.3231 acc_val: 0.6000 time: 0.0081s\n","Epoch: 0069 loss_train: 1.1964 acc_train: 0.6143 loss_val: 1.3124 acc_val: 0.6067 time: 0.0053s\n","Epoch: 0070 loss_train: 1.1768 acc_train: 0.6357 loss_val: 1.3019 acc_val: 0.6133 time: 0.0065s\n","Epoch: 0071 loss_train: 1.1553 acc_train: 0.6857 loss_val: 1.2913 acc_val: 0.6300 time: 0.0059s\n","Epoch: 0072 loss_train: 1.1507 acc_train: 0.6429 loss_val: 1.2810 acc_val: 0.6367 time: 0.0065s\n","Epoch: 0073 loss_train: 1.1586 acc_train: 0.6714 loss_val: 1.2706 acc_val: 0.6367 time: 0.0065s\n","Epoch: 0074 loss_train: 1.1230 acc_train: 0.6857 loss_val: 1.2602 acc_val: 0.6367 time: 0.0065s\n","Epoch: 0075 loss_train: 1.1054 acc_train: 0.6786 loss_val: 1.2501 acc_val: 0.6433 time: 0.0068s\n","Epoch: 0076 loss_train: 1.0864 acc_train: 0.6929 loss_val: 1.2404 acc_val: 0.6600 time: 0.0062s\n","Epoch: 0077 loss_train: 1.0677 acc_train: 0.6786 loss_val: 1.2310 acc_val: 0.6800 time: 0.0066s\n","Epoch: 0078 loss_train: 1.0663 acc_train: 0.6857 loss_val: 1.2220 acc_val: 0.7100 time: 0.0065s\n","Epoch: 0079 loss_train: 1.1130 acc_train: 0.6714 loss_val: 1.2129 acc_val: 0.7167 time: 0.0062s\n","Epoch: 0080 loss_train: 1.0515 acc_train: 0.7214 loss_val: 1.2037 acc_val: 0.7200 time: 0.0054s\n","Epoch: 0081 loss_train: 1.0715 acc_train: 0.7000 loss_val: 1.1946 acc_val: 0.7233 time: 0.0066s\n","Epoch: 0082 loss_train: 1.0330 acc_train: 0.7286 loss_val: 1.1855 acc_val: 0.7233 time: 0.0066s\n","Epoch: 0083 loss_train: 1.0041 acc_train: 0.7357 loss_val: 1.1766 acc_val: 0.7233 time: 0.0065s\n","Epoch: 0084 loss_train: 1.0292 acc_train: 0.7000 loss_val: 1.1683 acc_val: 0.7300 time: 0.0066s\n","Epoch: 0085 loss_train: 1.0368 acc_train: 0.7429 loss_val: 1.1603 acc_val: 0.7300 time: 0.0077s\n","Epoch: 0086 loss_train: 0.9656 acc_train: 0.7143 loss_val: 1.1527 acc_val: 0.7333 time: 0.0062s\n","Epoch: 0087 loss_train: 1.0505 acc_train: 0.7000 loss_val: 1.1449 acc_val: 0.7500 time: 0.0065s\n","Epoch: 0088 loss_train: 1.0379 acc_train: 0.7000 loss_val: 1.1378 acc_val: 0.7600 time: 0.0071s\n","Epoch: 0089 loss_train: 0.9589 acc_train: 0.7929 loss_val: 1.1306 acc_val: 0.7633 time: 0.0088s\n","Epoch: 0090 loss_train: 0.9475 acc_train: 0.7929 loss_val: 1.1232 acc_val: 0.7667 time: 0.0064s\n","Epoch: 0091 loss_train: 0.9588 acc_train: 0.7929 loss_val: 1.1159 acc_val: 0.7700 time: 0.0065s\n","Epoch: 0092 loss_train: 0.9477 acc_train: 0.7857 loss_val: 1.1081 acc_val: 0.7733 time: 0.0066s\n","Epoch: 0093 loss_train: 0.8401 acc_train: 0.8571 loss_val: 1.1000 acc_val: 0.7733 time: 0.0065s\n","Epoch: 0094 loss_train: 0.9355 acc_train: 0.8071 loss_val: 1.0918 acc_val: 0.7733 time: 0.0053s\n","Epoch: 0095 loss_train: 0.9162 acc_train: 0.7571 loss_val: 1.0841 acc_val: 0.7733 time: 0.0058s\n","Epoch: 0096 loss_train: 0.8869 acc_train: 0.7929 loss_val: 1.0768 acc_val: 0.7733 time: 0.0060s\n","Epoch: 0097 loss_train: 0.9333 acc_train: 0.7857 loss_val: 1.0696 acc_val: 0.7767 time: 0.0065s\n","Epoch: 0098 loss_train: 0.9095 acc_train: 0.7857 loss_val: 1.0626 acc_val: 0.7967 time: 0.0062s\n","Epoch: 0099 loss_train: 0.8640 acc_train: 0.8071 loss_val: 1.0561 acc_val: 0.8033 time: 0.0066s\n","Epoch: 0100 loss_train: 0.8760 acc_train: 0.8214 loss_val: 1.0503 acc_val: 0.7967 time: 0.0063s\n","Epoch: 0101 loss_train: 0.8598 acc_train: 0.8071 loss_val: 1.0447 acc_val: 0.7967 time: 0.0066s\n","Epoch: 0102 loss_train: 0.8270 acc_train: 0.8429 loss_val: 1.0388 acc_val: 0.8000 time: 0.0064s\n","Epoch: 0103 loss_train: 0.8165 acc_train: 0.8500 loss_val: 1.0326 acc_val: 0.7967 time: 0.0051s\n","Epoch: 0104 loss_train: 0.8467 acc_train: 0.8357 loss_val: 1.0262 acc_val: 0.7967 time: 0.0067s\n","Epoch: 0105 loss_train: 0.8508 acc_train: 0.8143 loss_val: 1.0192 acc_val: 0.7900 time: 0.0063s\n","Epoch: 0106 loss_train: 0.8493 acc_train: 0.8429 loss_val: 1.0122 acc_val: 0.7933 time: 0.0065s\n","Epoch: 0107 loss_train: 0.8718 acc_train: 0.7929 loss_val: 1.0051 acc_val: 0.7900 time: 0.0066s\n","Epoch: 0108 loss_train: 0.7798 acc_train: 0.8500 loss_val: 0.9978 acc_val: 0.7933 time: 0.0066s\n","Epoch: 0109 loss_train: 0.8176 acc_train: 0.8286 loss_val: 0.9900 acc_val: 0.7933 time: 0.0065s\n","Epoch: 0110 loss_train: 0.7730 acc_train: 0.8500 loss_val: 0.9825 acc_val: 0.7967 time: 0.0073s\n","Epoch: 0111 loss_train: 0.7962 acc_train: 0.8429 loss_val: 0.9751 acc_val: 0.7933 time: 0.0075s\n","Epoch: 0112 loss_train: 0.7889 acc_train: 0.8214 loss_val: 0.9673 acc_val: 0.7900 time: 0.0066s\n","Epoch: 0113 loss_train: 0.7777 acc_train: 0.8429 loss_val: 0.9611 acc_val: 0.7900 time: 0.0058s\n","Epoch: 0114 loss_train: 0.8123 acc_train: 0.7857 loss_val: 0.9556 acc_val: 0.7967 time: 0.0068s\n","Epoch: 0115 loss_train: 0.7409 acc_train: 0.8786 loss_val: 0.9502 acc_val: 0.7967 time: 0.0066s\n","Epoch: 0116 loss_train: 0.7527 acc_train: 0.8643 loss_val: 0.9441 acc_val: 0.7900 time: 0.0055s\n","Epoch: 0117 loss_train: 0.7237 acc_train: 0.8786 loss_val: 0.9381 acc_val: 0.7967 time: 0.0066s\n","Epoch: 0118 loss_train: 0.7129 acc_train: 0.8857 loss_val: 0.9325 acc_val: 0.7967 time: 0.0086s\n","Epoch: 0119 loss_train: 0.7205 acc_train: 0.8786 loss_val: 0.9270 acc_val: 0.7933 time: 0.0062s\n","Epoch: 0120 loss_train: 0.7477 acc_train: 0.8500 loss_val: 0.9221 acc_val: 0.7900 time: 0.0063s\n","Epoch: 0121 loss_train: 0.7466 acc_train: 0.8429 loss_val: 0.9163 acc_val: 0.7933 time: 0.0064s\n","Epoch: 0122 loss_train: 0.6981 acc_train: 0.8929 loss_val: 0.9094 acc_val: 0.8000 time: 0.0065s\n","Epoch: 0123 loss_train: 0.7169 acc_train: 0.8571 loss_val: 0.9023 acc_val: 0.7967 time: 0.0082s\n","Epoch: 0124 loss_train: 0.7292 acc_train: 0.8643 loss_val: 0.8968 acc_val: 0.8033 time: 0.0070s\n","Epoch: 0125 loss_train: 0.7243 acc_train: 0.8429 loss_val: 0.8921 acc_val: 0.8100 time: 0.0082s\n","Epoch: 0126 loss_train: 0.7114 acc_train: 0.8500 loss_val: 0.8881 acc_val: 0.8100 time: 0.0078s\n","Epoch: 0127 loss_train: 0.6743 acc_train: 0.8500 loss_val: 0.8846 acc_val: 0.8200 time: 0.0071s\n","Epoch: 0128 loss_train: 0.6696 acc_train: 0.8929 loss_val: 0.8810 acc_val: 0.8200 time: 0.0058s\n","Epoch: 0129 loss_train: 0.6893 acc_train: 0.8643 loss_val: 0.8769 acc_val: 0.8233 time: 0.0070s\n","Epoch: 0130 loss_train: 0.6634 acc_train: 0.8786 loss_val: 0.8728 acc_val: 0.8200 time: 0.0060s\n","Epoch: 0131 loss_train: 0.6334 acc_train: 0.9143 loss_val: 0.8686 acc_val: 0.8167 time: 0.0066s\n","Epoch: 0132 loss_train: 0.6891 acc_train: 0.8714 loss_val: 0.8647 acc_val: 0.8167 time: 0.0060s\n","Epoch: 0133 loss_train: 0.6452 acc_train: 0.9214 loss_val: 0.8608 acc_val: 0.8033 time: 0.0057s\n","Epoch: 0134 loss_train: 0.6665 acc_train: 0.8357 loss_val: 0.8569 acc_val: 0.8000 time: 0.0061s\n","Epoch: 0135 loss_train: 0.6731 acc_train: 0.9000 loss_val: 0.8530 acc_val: 0.8033 time: 0.0066s\n","Epoch: 0136 loss_train: 0.6574 acc_train: 0.8786 loss_val: 0.8488 acc_val: 0.7967 time: 0.0064s\n","Epoch: 0137 loss_train: 0.6119 acc_train: 0.8500 loss_val: 0.8448 acc_val: 0.8033 time: 0.0065s\n","Epoch: 0138 loss_train: 0.6206 acc_train: 0.8714 loss_val: 0.8404 acc_val: 0.8033 time: 0.0065s\n","Epoch: 0139 loss_train: 0.6527 acc_train: 0.8429 loss_val: 0.8373 acc_val: 0.8067 time: 0.0066s\n","Epoch: 0140 loss_train: 0.5865 acc_train: 0.8857 loss_val: 0.8349 acc_val: 0.8067 time: 0.0070s\n","Epoch: 0141 loss_train: 0.6227 acc_train: 0.8786 loss_val: 0.8334 acc_val: 0.8133 time: 0.0065s\n","Epoch: 0142 loss_train: 0.5525 acc_train: 0.9071 loss_val: 0.8325 acc_val: 0.8167 time: 0.0064s\n","Epoch: 0143 loss_train: 0.5876 acc_train: 0.8786 loss_val: 0.8312 acc_val: 0.8167 time: 0.0064s\n","Epoch: 0144 loss_train: 0.6368 acc_train: 0.8786 loss_val: 0.8294 acc_val: 0.8167 time: 0.0068s\n","Epoch: 0145 loss_train: 0.6687 acc_train: 0.8643 loss_val: 0.8250 acc_val: 0.8133 time: 0.0067s\n","Epoch: 0146 loss_train: 0.6351 acc_train: 0.8571 loss_val: 0.8208 acc_val: 0.8100 time: 0.0066s\n","Epoch: 0147 loss_train: 0.5597 acc_train: 0.9214 loss_val: 0.8169 acc_val: 0.8067 time: 0.0108s\n","Epoch: 0148 loss_train: 0.5836 acc_train: 0.8857 loss_val: 0.8130 acc_val: 0.8067 time: 0.0080s\n","Epoch: 0149 loss_train: 0.5921 acc_train: 0.8714 loss_val: 0.8092 acc_val: 0.7967 time: 0.0095s\n","Epoch: 0150 loss_train: 0.6424 acc_train: 0.8786 loss_val: 0.8049 acc_val: 0.7933 time: 0.0066s\n","Epoch: 0151 loss_train: 0.6434 acc_train: 0.8500 loss_val: 0.8013 acc_val: 0.8033 time: 0.0067s\n","Epoch: 0152 loss_train: 0.5803 acc_train: 0.8714 loss_val: 0.7977 acc_val: 0.8033 time: 0.0068s\n","Epoch: 0153 loss_train: 0.5535 acc_train: 0.9143 loss_val: 0.7941 acc_val: 0.8033 time: 0.0066s\n","Epoch: 0154 loss_train: 0.5410 acc_train: 0.9143 loss_val: 0.7909 acc_val: 0.8033 time: 0.0063s\n","Epoch: 0155 loss_train: 0.5603 acc_train: 0.9143 loss_val: 0.7883 acc_val: 0.8100 time: 0.0059s\n","Epoch: 0156 loss_train: 0.5695 acc_train: 0.9000 loss_val: 0.7862 acc_val: 0.8100 time: 0.0051s\n","Epoch: 0157 loss_train: 0.5454 acc_train: 0.8857 loss_val: 0.7836 acc_val: 0.8167 time: 0.0069s\n","Epoch: 0158 loss_train: 0.5524 acc_train: 0.9286 loss_val: 0.7811 acc_val: 0.8167 time: 0.0060s\n","Epoch: 0159 loss_train: 0.5469 acc_train: 0.8786 loss_val: 0.7788 acc_val: 0.8200 time: 0.0069s\n","Epoch: 0160 loss_train: 0.5991 acc_train: 0.8929 loss_val: 0.7764 acc_val: 0.8233 time: 0.0067s\n","Epoch: 0161 loss_train: 0.5468 acc_train: 0.9071 loss_val: 0.7748 acc_val: 0.8200 time: 0.0066s\n","Epoch: 0162 loss_train: 0.5103 acc_train: 0.9214 loss_val: 0.7723 acc_val: 0.8167 time: 0.0051s\n","Epoch: 0163 loss_train: 0.5527 acc_train: 0.9071 loss_val: 0.7701 acc_val: 0.8133 time: 0.0065s\n","Epoch: 0164 loss_train: 0.5312 acc_train: 0.9143 loss_val: 0.7669 acc_val: 0.8167 time: 0.0109s\n","Epoch: 0165 loss_train: 0.5011 acc_train: 0.9286 loss_val: 0.7635 acc_val: 0.8233 time: 0.0075s\n","Epoch: 0166 loss_train: 0.5181 acc_train: 0.9143 loss_val: 0.7609 acc_val: 0.8233 time: 0.0062s\n","Epoch: 0167 loss_train: 0.5211 acc_train: 0.8929 loss_val: 0.7583 acc_val: 0.8167 time: 0.0054s\n","Epoch: 0168 loss_train: 0.5006 acc_train: 0.9214 loss_val: 0.7555 acc_val: 0.8167 time: 0.0067s\n","Epoch: 0169 loss_train: 0.5649 acc_train: 0.8929 loss_val: 0.7527 acc_val: 0.8233 time: 0.0065s\n","Epoch: 0170 loss_train: 0.5049 acc_train: 0.9357 loss_val: 0.7496 acc_val: 0.8267 time: 0.0065s\n","Epoch: 0171 loss_train: 0.4689 acc_train: 0.9357 loss_val: 0.7473 acc_val: 0.8233 time: 0.0061s\n","Epoch: 0172 loss_train: 0.5020 acc_train: 0.9214 loss_val: 0.7453 acc_val: 0.8200 time: 0.0067s\n","Epoch: 0173 loss_train: 0.5206 acc_train: 0.9143 loss_val: 0.7432 acc_val: 0.8167 time: 0.0070s\n","Epoch: 0174 loss_train: 0.5102 acc_train: 0.9000 loss_val: 0.7410 acc_val: 0.8233 time: 0.0066s\n","Epoch: 0175 loss_train: 0.5265 acc_train: 0.8786 loss_val: 0.7393 acc_val: 0.8233 time: 0.0081s\n","Epoch: 0176 loss_train: 0.4787 acc_train: 0.9071 loss_val: 0.7382 acc_val: 0.8233 time: 0.0079s\n","Epoch: 0177 loss_train: 0.5457 acc_train: 0.9071 loss_val: 0.7377 acc_val: 0.8233 time: 0.0071s\n","Epoch: 0178 loss_train: 0.4853 acc_train: 0.9214 loss_val: 0.7377 acc_val: 0.8233 time: 0.0063s\n","Epoch: 0179 loss_train: 0.4884 acc_train: 0.9286 loss_val: 0.7379 acc_val: 0.8233 time: 0.0066s\n","Epoch: 0180 loss_train: 0.4813 acc_train: 0.9357 loss_val: 0.7386 acc_val: 0.8200 time: 0.0065s\n","Epoch: 0181 loss_train: 0.5047 acc_train: 0.9143 loss_val: 0.7383 acc_val: 0.8167 time: 0.0064s\n","Epoch: 0182 loss_train: 0.4867 acc_train: 0.9143 loss_val: 0.7368 acc_val: 0.8167 time: 0.0065s\n","Epoch: 0183 loss_train: 0.5076 acc_train: 0.8714 loss_val: 0.7336 acc_val: 0.8133 time: 0.0071s\n","Epoch: 0184 loss_train: 0.4759 acc_train: 0.9143 loss_val: 0.7301 acc_val: 0.8133 time: 0.0076s\n","Epoch: 0185 loss_train: 0.4754 acc_train: 0.9214 loss_val: 0.7266 acc_val: 0.8200 time: 0.0058s\n","Epoch: 0186 loss_train: 0.4363 acc_train: 0.9214 loss_val: 0.7231 acc_val: 0.8233 time: 0.0081s\n","Epoch: 0187 loss_train: 0.4995 acc_train: 0.9286 loss_val: 0.7195 acc_val: 0.8200 time: 0.0083s\n","Epoch: 0188 loss_train: 0.5159 acc_train: 0.9071 loss_val: 0.7166 acc_val: 0.8267 time: 0.0067s\n","Epoch: 0189 loss_train: 0.4757 acc_train: 0.9143 loss_val: 0.7149 acc_val: 0.8333 time: 0.0064s\n","Epoch: 0190 loss_train: 0.4488 acc_train: 0.9357 loss_val: 0.7138 acc_val: 0.8367 time: 0.0067s\n","Epoch: 0191 loss_train: 0.4585 acc_train: 0.9357 loss_val: 0.7125 acc_val: 0.8367 time: 0.0064s\n","Epoch: 0192 loss_train: 0.4610 acc_train: 0.9000 loss_val: 0.7118 acc_val: 0.8333 time: 0.0067s\n","Epoch: 0193 loss_train: 0.4353 acc_train: 0.9286 loss_val: 0.7123 acc_val: 0.8267 time: 0.0058s\n","Epoch: 0194 loss_train: 0.4430 acc_train: 0.9571 loss_val: 0.7136 acc_val: 0.8267 time: 0.0068s\n","Epoch: 0195 loss_train: 0.4705 acc_train: 0.9071 loss_val: 0.7146 acc_val: 0.8300 time: 0.0068s\n","Epoch: 0196 loss_train: 0.4382 acc_train: 0.9500 loss_val: 0.7163 acc_val: 0.8267 time: 0.0065s\n","Epoch: 0197 loss_train: 0.4247 acc_train: 0.9357 loss_val: 0.7174 acc_val: 0.8267 time: 0.0064s\n","Epoch: 0198 loss_train: 0.4747 acc_train: 0.9214 loss_val: 0.7168 acc_val: 0.8267 time: 0.0065s\n","Epoch: 0199 loss_train: 0.4333 acc_train: 0.9214 loss_val: 0.7146 acc_val: 0.8267 time: 0.0065s\n","Epoch: 0200 loss_train: 0.4473 acc_train: 0.9214 loss_val: 0.7109 acc_val: 0.8300 time: 0.0054s\n","Optimization Finished!\n","Total time elapsed: 1.5994s\n","Test set results: loss= 0.7688 accuracy= 0.8280\n"]}]}]}